---
title: "Case-study-hypertension"
author: "Hanchao Zhang, M.S."
output:
  html_document:
    md_extensions: -startnum
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(warning = F)

#if(!require("nhanesA")){
#  install.packages(c("nhanesA", "tidyverse", "knitr", "sas7bdat", "tableone" ))
#}
```

\newpage

## Motivation


The pathology of hypertension has been found with several medical studies, salt intaking is one of the most important risk factors for having hypertension. Instead of seeing the association of hypertension and salt intake, we focus on other factors that might have the impact on the odds of having hypertension.

Moreover, our case study also introduced the logistic regression and survey-weighted logistic regression, focusing on the difference of the two models and obviously survey-weighted logistic regression would be our choice of the model since we are using the survey data and it is weighted.




## What is the Data

The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey started in the 1970â€™s and became a continuous program since 1999. A large portion of the data is publicly available at https://wwwn.cdc.gov/nchs/nhanes/default.aspx. The R package nhanesA (https://cran.r-project.org/web/packages/nhanesA/vignettes/Introducing_nhanesA.html) may make it easier to explore the NHANES data for surveys conducted since 1999. Additionally, the linked mortality data are available for NHANES III and later surveys at this site: https://www.cdc.gov/nchs/data-linkage/mortality-public.htm, which will be useful for exploring questions related to survival outcomes of the survey participants.

The data is downloaded by NYC NHANES (a local version of NHANES), The NYC Health and Nutrition Examination Survey (NYC HANES), modeled on the National Health and Nutrition Examination Survey, is a population-based, cross-sectional study with data collected from a physical examination and laboratory tests, as well as a face-to-face interview and an audio computer-assisted self-interview (ACASI).(cited from http://nychanes.org/data/)



## Data Preprocessing

### Load packages
```{r, warning=F, message=F}
if(!require("nhanesA")){
  install.packages(c("nhanesA", "tidyverse", "knitr", "sas7bdat", "tableone", 'survey' ))
}
library(tidyverse)
library(knitr)
library(sas7bdat)
library(tableone)
library(knitr)
library(kableExtra)
library(survey)
library(broom)
```


### Read the Data from csv File



```{r}
dat <- read.sas7bdat('/Users/hanchao/GitHub/Case-study/open-case-studies-Hypertension/Data/dat.sas7bdat')
```

### Select the Variables that You're Interesed In


For the covariates of interest, we check the related paper and selected 11 covariates that have a potential association with hypertension according to paper reviews. The variables can also be chosen by univariate analysis in the whole data set with adjusted family error rates. We will talk about the way of doing that in the other project.


```{r}
hypertension_DF <- data.frame(
id = dat$KEY,
age = dat$SPAGE,
race = dat$DMQ_14_1,
gender = dat$GENDER,
born = dat$US_BORN,
diet = dat$DBQ_1,
income = dat$INC20K,
diabetes = dat$DX_DBTS,
bmi = dat$BMI,
hypertension = dat$BPQ_2,
drink = dat$ALQ_1_UNIT,
smoking = dat$SMOKER3CAT,
surveyweight = dat$CAPI_WT
)
```

### Adjust the Data Type


After you get you clean data set, adjusting the type of the covariate is very important, and you should do before doing any analysis in R.

Sometimes people forget to change the type of data, and usually, it will give you a totally wrong result.

There are several ways that you can have a glance at your data. Plotting the data is an excellent way to understand your data at first sight.


```{r, fig.height=6}
plot(hypertension_DF)


par(mfrow = c(3,4))
for (i in names(hypertension_DF)[-1]) {
  hist(hypertension_DF[,i], main = i, xlab = i)
}


```





The other way to understand the data better is using the summary and str functions to see the data summary



```{r}
## check the original data type
str(hypertension_DF)
## find a charactor of the data for further adjustment
summary(hypertension_DF)

```




From the two figures above, we can tell that there are some categorical variables like race, gender, born place, diet, neighborhood incomes, income, diabetes, BMI, drink, and smoke.

So, the first step, we need to, instead of using the numeric class of it, change it to factors.

We can, of course, change it one by one. However, the reason that we plot the data and look at the summary of it first is that it can give us some information so that we can process the data easier.

A better way to change the type of data especially when you have lots of covariates is to set a threshold according to the character of the data. Here, we can see that the mean of the categorical data cannot exceed 5. So, we set 5 as the threshold and for any variable with mean smaller than 5, we set it to factor, else, we maintain its type.




```{r}
factorvars <- NULL
## if the mean value of the data is smaller than 4, 
## it means these data is very likely to be categorical data, adjust it to factor.
for (i in 2:length(hypertension_DF)) {
  hypertension_DF[hypertension_DF[,i] == 'NaN',i] <- NA
  if( mean(hypertension_DF[,i], na.rm = T) < 5 ){
    ## For NaN, change it to missing value first
    #hypertension_DF[hypertension_DF[,i] == 'NaN',i] <- NA
    hypertension_DF[,i] <- as.factor(hypertension_DF[,i])
    ## Record the categorical data
    factorvars[i] <- names(hypertension_DF)[i]
  }
}
## check the changed data type
str(hypertension_DF)
```

### Rename the Levels of the Factor

As the str function shows, we now have our categorical variable in position. However, it's kind of hard for us to tell what the different levels represent in categorical variables. So we can rename the level and make it easier to read and analyze later.



```{r}
hypertension_DF$race <- as.factor(hypertension_DF$race)
levels(hypertension_DF$race) <- c('White', 'Black/African American', 'Indian (American)/Alaska Native', 'Native Hawaiian/Other Pacific Islander', 'Asian', 'Some Other Race')
levels(hypertension_DF$gender) <- c('Male', 'Female')
levels(hypertension_DF$born) <- c('US-born', 'Other')
levels(hypertension_DF$diet) <- c('Excellent', 'Very good', 'Good', 'Fair', 'Poor')
levels(hypertension_DF$income) <- c('Less than $20,000','$20,000 - $39,999','$40,000 - $59,999','$60,000 - $79,999','$80,000 - $99,999','$100,000 or more')
levels(hypertension_DF$diabetes) <- c('Diabetic dx', 'Diabetic but no dx', 'Not diabetic')
levels(hypertension_DF$hypertension) <- c('Yes', 'No')
levels(hypertension_DF$drink) <- c('Weekly', 'Monthly', 'Yearly')
levels(hypertension_DF$smoking) <- c('Weekly', 'Monthly', 'Yearly')
```








## Exploratory Data Analysis

### Check Normality of the Numeric Variables


After changing the dataset, we are now, having a clean dataset. To explore the data more, we need to do some analysis on the data and to check some other features of them.

One thing that we want to do first is to check the normality of your continuous data because we have to decide what test statistics to use. For example, we use the t-test to check the difference of the normal distribution and Wilcoxon test to check the difference of non-normal distribution.


```{r}
par(mfrow = c(1,2))
for (i in names(hypertension_DF[,-ncol(hypertension_DF)])) {
  if( is.numeric(hypertension_DF[,i] ) ){
    qqnorm(hypertension_DF[,i], main = i)
    qqline(hypertension_DF[,i])
  }
}
```


Since the age and BMI are not normally distributed, we should use the non-parametric(Wilcoxon) method to test the differences.


### Demographic


Demographic of the data can help you to check whether the difference of your covariates of interest is significant by stratifying the interest of outcome variables. Essentially, we are doing the t-test(normal distribution), chi-sq(categorical data) test, and Wilcoxon(non-normal distribution) test for each of the covariates.

Of course, you can do it by formula or function like 't.test'. Here we introduce a simple function that helps you to get the test statistics and a nice and decent demographic table.

CreateTableOne is a function from tableone package. We need to specify the data you are using, the outcome variable as strata, the covariates that you want to test as vars (default is all variables expect the outcome variable). We also need to specify the categorical data as factorVars and non-normal distributed data as nonnormal in the function.


```{r, results='hide'}
CreateTableOne(data = hypertension_DF[,-1], strata = c('hypertension'), factorVars = factorvars) %>%
  print(nonnormal = c('age','BMI')) -> tblprint
```

```{r}
kable(tblprint[,-4])
```



**Question: are we doing it right?**

If no, why and what should we do?

### What is Survey Weighted Data

It possible and indeed often happens to a perfectly designed sampling plan ends up with too many samples in a category. For example, too many women and not enough men, or too many white and not enough other races or both. Data weighting make sense for this kind of data. If we want the data to reflect the whole population, instead of counting the weight of each data point as one, we weight the data so that the sample we have can better reflect the entire community.



### What is the Weight of Data


Assuming that you have 25 students (20 male and 5 female) in your class, and you want to talk with 5 of them to know their understanding of the biostatistics class. By sampling 5 students from the total 25 students, you might get 5 all female students or 4 female and 1 male in your sample. Do you expect this sample to represent the population? Of course not. That's why we have to weight the observation so that we can make the sample better represent the population. The way of calculating the weight is:



$$Weight = \frac{Proportion~in~population}{Proportion~in~sample}$$
$$Male~Weight = \frac{20/25}{1/25} = 20$$
$$Male~Weight = \frac{5/25}{4/25} = 1.25$$


When we have multiple strata on the data, it might be troublesome to calculate the weight. However, for most of the survey data, the weight is calculated and included in the dataset. In our case study, the weight is calculated and we can simply apply the weight in the survey-weighted logistic regression.

### Survey Weighted Demographic table

Luckily, the Createtableone function also has a survey-weighted version. We can use the svyCreateTableOne function output the demographic table for survey weighted data.

Before doing that, we need to create our design first by the svydesign which we will introduce in detail later in the regression part.


```{r}
fpc <- ((5827719 - 1527)/(5827719-1))^0.5
fpc
```

```{r}
hypertension <- hypertension_DF$hypertension
hypertension <- ifelse(hypertension == 'No',0,1)
hypertension_design <- svydesign(
  id = ~1,
  data = hypertension_DF[,2:ncol(hypertension_DF)],
  weights = ~surveyweight,
  fpc = ~rep(fpc, nrow(hypertension_DF))
)
```


Similarly, we use the same strata, factorVars and nonnormal to identify the outcome variable, categorical variables and non-normal distributed continuous variables.



```{r, results='hide'}
svyCreateTableOne(data = hypertension_design, strata = 'hypertension', factorVars = factorvars) %>%
  print(nonnormal = c('age','BMI')) -> tblprint2
```


```{r}
kable(tblprint2 [-c(30,39),])

```


As we can see, we have different results. First, the number of the observation changed to the total number of the population that we input before. Moreover, the p-value of the gender become significant. The survey-weighted demographic should be the one that we use for survey weighted data analysis.


We can also do it in another way. Univariates analysis can replace the result of the survey-weighted demographic table.


```{r}
hypertension_design$variables$hypertension <- ifelse(hypertension_design$variables$hypertension == 'No', yes = 0, no = 1)

pvalue <- list()
for (i in names(hypertension_design$variables)) {
  svyglm_fit <- svyglm(formula(paste('hypertension~', i)), hypertension_design)
  svyglm_summary <- summary(svyglm_fit)
  p <- round(svyglm_summary$coefficients[,4],3)
  p <- ifelse(p < 0.001, yes = '< 0.001', no = p)
  pvalue[[i]] <- p
}

pvalue

```


## Data analysis


### Logistic Regression

After checking the test-statistics, we can then build our model on our data.

Since the outcome variables are binary categorical variable, logistic regression will be fit by our data.


#### What is Logistic Regression?

+ logistic function

$$\eta(Z) = \frac{1}{1+e^{-z}}$$

+ inverse of the logistic function

$$\eta^{-1} = log(\frac{z}{1-z})$$


$$log(\frac{p}{1-p}) = {(\beta^TX)}$$

#### Why Logistic Regression?

Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary). 

#### Assumptions of Logistic Regression

Unlike the linear Gaussian model,

+ We are not assuming there is a linear relationship between the outcome variable and the logit of the linear combination of the covariates. It less strick comparing with linear regression

+ The error terms (residuals) do not need to be normally distributed

+ Homoscedasticity is not required.  Finally, the dependent variable in logistic regression is not measured on an interval or ratio scale





#### The loss function of logistic regression (Cross-entropy)

$$H(Q) = -\sum_z P(z)logQ(z)$$

where z is the discrete random variable with probability density function P, and the Q is the candidate density function of P.

$$H(x;\beta) = -\pi(x)log[p(x;\beta)] - [1-\pi(x)]log[1-p(x;\beta)]$$
By minimizing this the cross-entropy, we can have our $\beta$ calcuated by gradience descent. Where

$$\hat \beta^{new} = \hat \beta^{old} + (X^TWX)^{-1}X^T[Y-p(X;\hat \beta^{old})]$$


#### What does Logistic Function Look Like?

```{r}
eta <- function(z){1/(1+exp(-z))}
curve(eta, xlim = c(-6,6), ylab = 'eta(z)', xlab = 'z', main = "Logistic Function")
```


#### Logistic Regression

```{r}
norglm <- glm(hypertension ~  + . - surveyweight  , data = na.omit(hypertension_DF[,-1]), family = binomial(link = "logit")) 
norglm_summary <- summary(norglm)
norglm_tbl <- norglm_summary$coefficients
norglm_tbl <- cbind(norglm_tbl, confint(norglm))
norglm_tbl <- norglm_tbl[,c(1,5,6,4)]

norglm_tbl <- cbind( (apply(norglm_tbl[,1:3], 2, exp) ),
      'P-value' = norglm_tbl[,4])

norglm_tbl[,1:4] <- apply(norglm_tbl[,1:4], 2, function(x)round(x,3))
norglm_tbl <- data.frame(norglm_tbl)
norglm_tbl[,4][norglm_tbl[,4] < 0.001] <- '< 0.001'
names(norglm_tbl) <- c('Odds Ratio','2.5% CI','97.5% CI','P-value')
kable(norglm_tbl, caption = "Logistic Regression")
```

#### Summary of Results

In summary, we can interpret the association between the outcome variables and covariates with odds ratio. Recite that odds ration is $\frac{p}{1-p}$ p as the probability of having hypertension.

The odds of having hypertension is 0.95 ($p<0.001$) times larger with every unit increase in the age. (Does it make sense to you? Think about why we have this kind of counterintuitive result)

The odds of having hypertension is 1.9 ($p<0.001$) times larger for men compared to women.

The odds of having hypertension is 3.3 ($p<0.001$) times larger for people with income \$50,000 - \$74,999 than with income less than \$20,000

The odds of having hypertension is 2.9 ($p<0.001$) times larger for people with income more than \$100,000 than with income less than \$20,000

The odds of having hypertension is 2.9 ($p<0.001$) times larger for people refuse to tell their income than with income less than \$20,000

Since the smoking and drinking are excluded from the model and for the full model, the smoking and drinking status are both not statistically significant ($p>0.05$), we might conclude from our analysis that smoking and drinking do not expose people more on the hypertension, probably those people are essentially sicker than others (for example having diabetes is one reason) so that they are intuitively more likely to have hypertension. Does this conclusion make sense to you?




### Survey Weighted Logistic Regression


#### What is Finite Population Correction Factor

$$FPC = (\frac{N-n}{N-1})^{\frac{1}{2}}$$
+ N = population size
+ n = sample size

The Finite Population Correction Factor (FPC) is used when you sample without replacement from more than 5% of a finite population. Itâ€™s needed because under these circumstances, the Central Limit Theorem doesnâ€™t hold and the standard error of the estimate (e.g. the mean or proportion) will be too big. In basic terms, the FPC captures the difference between sampling with replacement and sampling without replacement.

Most real-life surveys involve finite populations sampled without replacement. For example, you might perform a telephone survey of 10,000 people; once a person has been called, they wonâ€™t be called again. (https://www.statisticshowto.datasciencecentral.com/finite-population-correction-factor/)



```{r}
fpc <- ((5827719 - 1527)/(5827719-1))^0.5
fpc
```

In our case, the fpc is pretty close to 1, so we do not necessarily need to consider it in our model.



#### Design the Survey

First, we need to use some of the concepts that we introduced above to design the survey.



```{r}
hypertension <- hypertension_DF$hypertension
hypertension <- ifelse(hypertension == 'No',0,1)
hypertension_design <- svydesign(
  id = ~1,
  data = hypertension_DF,
  weights = ~surveyweight,
  fpc = ~rep(fpc, nrow(hypertension_DF))
)
```

#### Fit the Survey Weighted Logistic Regression



By using svyglm function, we can fit the survey-weighted logistic regression. However, before doing so, you should make sure that your outcome variable is number instead of factor


```{r}
hypertension_design$variables$hypertension <- ifelse(hypertension_design$variables$hypertension == 'No', 0 , 1)
svyglm <- svyglm(hypertension ~ age + race + gender + born + diet + income + diabetes + bmi + drink + smoking, hypertension_design)
svyglm_summary <- summary(svyglm)

```



After that, we output the confidence interval and output a readable table



```{r}
svyglm_summary$coefficients[,1] <- exp(svyglm_summary$coefficients[,1])
colnames(svyglm_summary$coefficients)[1] <- 'Odds Ratio'
#svyglm_summary$coefficients[,4][svyglm_summary$coefficients[,4] < 0.001 ] <- '< 0.001'
svyglm_summary$coefficients %>%
  tidy() -> svyglm_tbl
svyglm_tbl %>%
  mutate(lower = exp(confint(svyglm)[,1])) %>%
  mutate(upper = exp(confint(svyglm)[,2])) -> svyglm_tbl
svyglm_tbl <- svyglm_tbl[,c(1,2,6,7,5)]
names(svyglm_tbl) <- c('Variables', 'Odds Ratio', '2.5% CI', '97.5% CI', 'P-value')
svyglm_tbl$`P-value`[svyglm_tbl$`P-value` < 0.001] <- '< 0.001'

svyglm_tbl[,2:4] <- apply(svyglm_tbl[,2:4],2, function(x)round(x,3))
svyglm_tbl %>%
  kable(caption = 'Survey Weighted Logistic Regression')
```


#### Summary of the results

As we can see from the table above, aftering adjusting the weight of the regression, the result changed dramatically. Every covariates become statistically significant in the model ($P < 0.001$).

Some of the interpretation:

The odds of having hypertension is 1.07 ($p<0.001$) times larger with every unit increase in the age. (Does it make sense to you? Think about why we have this kind of counterintuitive result)

The odds of having hypertension is 1.07 ($p<0.001$) times larger for Black people compared to White people

The odds of having hypertension is 0.92 ($p<0.001$) times larger for Indian and Alaska Native American compared to White people

The odds of having hypertension is 1.03 ($p<0.001$) times larger for Asian compared to White

The odds of having hypertension is 1.07 ($p<0.001$) times larger for some other race people compared to White people

The odds of having hypertension is 0.93 ($p<0.001$) times larger for women compared to men.


## Summary

When doing analysis using survey weighted data, there are some takeaway points:

+ When doing demographic, we need to use the survey-weighted version of the demographic table. We can also do univariates analysis with survey-weighted logistic regression. However, we do not recommend it

+ While doing the regression analysis, the survey-weighted version of the logistic regression should be applied using the survey package

+ We also need the set up the survey design first by the svydesign function by identifying the strata, weights, fpc parameters, and ids








