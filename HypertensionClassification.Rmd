---
title: "Case-study-hypertension-classification"
author: "Hanchao Zhang, M.S."
output:
  html_document:
    md_extensions: -startnum
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(warning = F)

#if(!require("nhanesA")){
#  install.packages(c("nhanesA", "tidyverse", "knitr", "sas7bdat", "tableone" ))
#}
```

\newpage


## Motivation


The pathology of hypertension has been found with several medical studies, salt intaking is one of the most important risk factors for having hypertension. Instead of seeing the association of hypertension and salt intake, we focus on other factors that might have the impact on the odds of having hypertension.

Moreover, our case study also introduced the logistic regression and survey-weighted logistic regression, focusing on the difference of the two models and obviously survey-weighted logistic regression would be our choice of the model since we are using the survey data and it is weighted.




## What is the Data

The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey started in the 1970â€™s and became a continuous program since 1999. A large portion of the data is publicly available at https://wwwn.cdc.gov/nchs/nhanes/default.aspx. The R package nhanesA (https://cran.r-project.org/web/packages/nhanesA/vignettes/Introducing_nhanesA.html) may make it easier to explore the NHANES data for surveys conducted since 1999. Additionally, the linked mortality data are available for NHANES III and later surveys at this site: https://www.cdc.gov/nchs/data-linkage/mortality-public.htm, which will be useful for exploring questions related to survival outcomes of the survey participants.

The data is downloaded by NYC NHANES (a local version of NHANES), The NYC Health and Nutrition Examination Survey (NYC HANES), modeled on the National Health and Nutrition Examination Survey, is a population-based, cross-sectional study with data collected from a physical examination and laboratory tests, as well as a face-to-face interview and an audio computer-assisted self-interview (ACASI).(cited from http://nychanes.org/data/)



## Data Preprocessing

### Load packages
```{r, warning=F, message=F}
if(!require("nhanesA")){
  install.packages(c("nhanesA", "tidyverse", "knitr", "sas7bdat", "tableone", 'survey' ))
}
library(tidyverse)
library(knitr)
library(sas7bdat)
library(tableone)
library(knitr)
library(kableExtra)
library(broom)
library(caret)
library(rpart)
library(glmnet)
library(pROC)
library(randomForest)


```


### Read the Data from csv File



```{r}
directory <- getwd()
directory
dat <- read.sas7bdat(paste0(directory,'/dat.sas7bdat'))
```

### Select the Variables that You're Interesed In


For the covariates of interest, we check the related paper and selected 11 covariates that have a potential association with hypertension according to paper reviews. The variables can also be chosen by univariate analysis in the whole data set with adjusted family error rates. We will talk about the way of doing that in the other project.


```{r}
hypertension_DF <- data.frame(
id = dat$KEY,
age = dat$SPAGE,
race = dat$DMQ_14_1,
gender = dat$GENDER,
born = dat$US_BORN,
diet = dat$DBQ_1,
income = dat$INC20K,
diabetes = dat$DX_DBTS,
bmi = dat$BMI,
hypertension = dat$BPQ_2,
drink = dat$ALQ_1_UNIT,
smoking = dat$SMOKER3CAT,
surveyweight = dat$CAPI_WT
)
```

### Adjust the Data Type


After you get you clean data set, adjusting the type of the covariate is very important, and you should do before doing any analysis in R.

Sometimes people forget to change the type of data, and usually, it will give you a totally wrong result.

For example, in ` lm()` and ` glm()`, the function will automatically make the factor to categorical data and if you didn't change the type of the data, you will have the wrong result which identify the categorical variables to continous variables

There are several ways that you can have a glance at your data. Plotting the data is an excellent way to understand your data at first sight.



There are several ways that you can have a glance at your data. What I would recommend you is using the summary and str functions to see the data summary





```{r}
## check the original data type
str(hypertension_DF)
```

The ` str()` function will return you the name of the variables along with their type and some values. It helps you to check the levels and missing values in the categorical data.





```{r}
## find a charactor of the data for further adjustment
summary(hypertension_DF)
```




From the two figures above, we can tell that there are some categorical variables like race, gender, born place, diet, neighborhood incomes, income, diabetes, BMI, drink, and smoke.

So, the first step, we need to, instead of using the numeric class of it, change it to factors.

We can, of course, change it one by one. However, the reason that we plot the data and look at the summary of it first is that it can give us some information so that we can process the data easier.

A better way to change the type of data especially when you have lots of covariates is to set a threshold according to the character of the data. Here, we can see that the mean of the categorical data cannot exceed 5. So, we set 5 as the threshold and for any variable with mean smaller than 5, we set it to factor, else, we maintain its type.




```{r}
factorvars <- NULL
## if the mean value of the data is smaller than 4, 
## it means these data is very likely to be categorical data, adjust it to factor.
for (i in 2:length(hypertension_DF)) {
  hypertension_DF[hypertension_DF[,i] == 'NaN',i] <- NA
  if( mean(hypertension_DF[,i], na.rm = T) < 5 ){
    ## For NaN, change it to missing value first
    #hypertension_DF[hypertension_DF[,i] == 'NaN',i] <- NA
    hypertension_DF[,i] <- as.factor(hypertension_DF[,i])
    ## Record the categorical data
    factorvars[i] <- names(hypertension_DF)[i]
  }
}
## check the changed data type
str(hypertension_DF)
```

### Rename the Levels of the Factor

As the str function shows, we now have our categorical variable in position. However, it's kind of hard for us to tell what the different levels represent in categorical variables. So we can rename the level and make it easier to read and analyze later.



```{r}
hypertension_DF$race <- as.factor(hypertension_DF$race)
levels(hypertension_DF$race) <- c('White', 'Black/African American', 'Indian (American)/Alaska Native', 'Native Hawaiian/Other Pacific Islander', 'Asian', 'Some Other Race')
levels(hypertension_DF$gender) <- c('Male', 'Female')
levels(hypertension_DF$born) <- c('US-born', 'Other')
levels(hypertension_DF$diet) <- c('Excellent', 'Very good', 'Good', 'Fair', 'Poor')
levels(hypertension_DF$income) <- c('Less than $20,000','$20,000 - $39,999','$40,000 - $59,999','$60,000 - $79,999','$80,000 - $99,999','$100,000 or more')
levels(hypertension_DF$diabetes) <- c('Diabetic dx', 'Diabetic but no dx', 'Not diabetic')
levels(hypertension_DF$hypertension) <- c('Yes', 'No')
levels(hypertension_DF$drink) <- c('Weekly', 'Monthly', 'Yearly')
levels(hypertension_DF$smoking) <- c('Weekly', 'Monthly', 'Yearly')
```




### Check Normality of the Numeric Variables


After changing the dataset, we are now, having a clean dataset. To explore the data more, we need to do some analysis on the data and to check some other features of them.

One thing that we want to do first is to check the normality of your continuous data because we have to decide what test statistics to use. For example, we use the t-test to check the difference of the normal distribution and Wilcoxon test to check the difference of non-normal distribution.


```{r}
par(mfrow = c(1,2))
for (i in names(hypertension_DF[,-ncol(hypertension_DF)])) {
  if( is.numeric(hypertension_DF[,i] ) ){
    qqnorm(hypertension_DF[,i], main = i)
    qqline(hypertension_DF[,i])
  }
}
```


Since the age and BMI are not normally distributed, we should use the non-parametric(Wilcoxon) method to test the differences.



## Caret Package

After preprocessed the data, we want to build a best prediction model. We will fit logistic regression, random forest and elastic net models first, and then compare the cross-validation error to select the best prediction model.

We will introduce the `caret` package first. This package gives us a simple way to compare cross-validation errors


### Set up a train control for cross-validation using ` trainControl()` function from ` caret` pacakge.

+ since we are using cross-validation, we will input the argument ` method = 'cv'` with number of the fold equal to 10

This is a setting in the later function ` train()` where we can train our models and output cross-validation error

```{r}
train_control<- trainControl(method="cv", number=3)
```


using the ` tran()` function with ` method = 'glm'`, we get the logistic regression with cross-validation. The output RMSE is the rooted mean squre error, we can power it by 2 to get the mean square error.

```{r}
hypertension_DF$hypertension <- ifelse(hypertension_DF$hypertension == 'No', yes = 0, no = 1)
hypertension_DF <- na.omit(hypertension_DF)
fit_glm <- train(hypertension~., data=hypertension_DF, trControl=train_control, method="glm")

fit_glm$results$RMSE^2

```

similarly, using ` method = 'rf'` and ` tuneLength = `5` to perform random forest with ramdom 15 tunning parameters mtry

```{r}
fit_rf <- train(hypertension~., data=hypertension_DF, trControl=train_control, method="rf", tuneLength = 15)
data.frame(
mtry = fit_rf$results$mtry,
mse = fit_rf$results$RMSE^2
) %>%
  arrange(mse) -> rf.mse
rf.mse

plot(fit_rf)
```

### Cross-validation

We can also perform cross-validation by hard coding. It is pretty simple after understanding the idea of cross-validation.

1. we split the data into k partitions, and we call it k-folds

2. we select one fold as validation dataset, the other k-1 folds as training dataset

3. fit the model using training dataset

4. estimate the error using the validation dataset

5. repeat with each fold, we can have k validation errors


We use elastic net as our exmaple to do the cross-validation hard coding

```{r}
set.seed(1234)
cv.shrinkage <- function(data = dat.reg, n = "fold", y = "Class", a = 1){
  dd <- data[sample(nrow(data), nrow(data), replace = F),]
  dat.splited <- split(dd, rep(1:n, each=nrow(dd)/(n)))
  best.l <- NULL
  fold.error <- NULL
      for (i in 1:n) {
      train <- dat.splited[-i]
      train <- do.call(rbind, train)
      valid <- dat.splited[[i]]
      cv.lasso <- cv.glmnet(x = model.matrix(hypertension ~., train), y = as.matrix(train[,y]), nfolds = n, alpha=a, type.measure="deviance", family = "binomial", standardize = T)
      l <- cv.lasso$lambda[which.min(cv.lasso$cvm)]
      fit.lasso <- glmnet(x = model.matrix(hypertension ~., train), y = as.matrix(train[,y]) , family = "binomial", alpha = a, lambda = l)
      pred <- predict(fit.lasso, newx = model.matrix(hypertension ~., valid), y = as.matrix(valid[,y]), type = "response")
      dat.roc <- roc(valid[,y]~pred)
      thres <- coords(dat.roc, "best", ret = "threshold")
      pred[pred <= thres] <- 0
      pred[pred > thres] <- 1
      best.l[i] <- l
      fold.error[i] <- mean(pred != valid[,y])
      }
  data.frame(
    lambda = mean(best.l),
    error = mean(fold.error)
  )
}

dadada <- list()

for (i in 1:length(seq(0,1, by = 0.1))) {
  dadada[[i]] <-tryCatch({
    cv.shrinkage(data = hypertension_DF[,-1], n = 10, y = "hypertension", a = seq(0,1, by = 0.1)[i])
  }, error = function(x)NA)
}


tbl.shrinkage <- cbind(
  alpha = (seq(0,1, by = 0.1)),
  do.call(rbind, dadada))

tbl.shrinkage %>%
  xtable::xtable(digits = 4)

tbl.shrinkage[which.min(tbl.shrinkage$error),] %>%
  xtable::xtable(digits = 4)

#tbl.shrinkage$error <- sqrt(tbl.shrinkage$error)

ggplot(tbl.shrinkage) +
  geom_line(aes(x = alpha, y = sqrt(error)), color = 'blue') +
  geom_point(aes(x = alpha, y = sqrt(error), label = lambda)) + 
  geom_text(aes(x = alpha, y = sqrt(error),  label = round(tbl.shrinkage$lambda, 4)), size = 3.5, hjust = 1,vjust = -0.5) +
  ylab('RMSE(Cross-Validation)')


```


The x is the alpha, y is the RMSE, and the point with value is the optimal lambda that return at the alpha.


```{r}
tbl.shrinkage[which.min(tbl.shrinkage$error),]

rf.mse[1,]

fit_glm$results$RMSE^2
```

Comparing with the cross-validation error, we get the best prediction model is the random forest model with mtry = 144.

## Final Prediction model 

Now, we can take a look on the random forest model that we fit


```{r}
fit_rf <- randomForest(hypertension~., data = hypertension_DF[,-c(1,13)], mytry = 144, tree = 1000)
fit_rf
```


```{r}

importance(fit_rf)

par(mfrow = c(1,2))
plot(fit_rf)
varImpPlot(fit_rf)
```

The error does not decrease too much as the number of the tree get to 200. The importance of plot define the importance of each variable by the reduction of the variance. The age reduce most of the variance following with bmi, income, diet, race. diabetes, drinking status, smoking status, gender and born place.






